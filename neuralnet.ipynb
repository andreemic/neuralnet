{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "built-jamaica",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "20f3a546",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "labeled-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self):\n",
    "        self.output = 0\n",
    "    def activate(self, inputs: np.ndarray, weights: np.ndarray):\n",
    "        logit = np.dot(inputs, weights)\n",
    "        self.output = sigmoid(logit)\n",
    "        return self.output\n",
    "\n",
    "class ConstantNeuron(Neuron):\n",
    "    def __init__(self, val):\n",
    "        super(Neuron, self).__init__()\n",
    "        self.output = val\n",
    "    def activate(self, inputs: np.ndarray, weights: np.ndarray):\n",
    "        return self.output\n",
    "    def set(self, val):\n",
    "        self.output = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "stuffed-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, size):\n",
    "        self.neurons = [Neuron() for i in range(size)]\n",
    "        self.y = np.zeros(size)\n",
    "        self.size = size\n",
    "    def propagate(self, inputs: np.ndarray, weights: np.ndarray):\n",
    "        # inputs: vector of values (x)\n",
    "        # weights: matrix where weights[i][j] is the weight going into i-th neuron from j-th input\n",
    "        for i in range(len(self.neurons)):\n",
    "            neuron = self.neurons[i]\n",
    "            # compute output for each neuron\n",
    "            self.y[i] = neuron.activate(inputs, weights[i])\n",
    "        return self.y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "established-hours",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # weights is an array of weight matricies\n",
    "        self.weights = []\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.input_layer = Layer(input_size)\n",
    "        self.output_layer = Layer(output_size)\n",
    "        self.layers = [self.input_layer, self.output_layer]\n",
    "        \n",
    "    def add_layer(self, layer: Layer):\n",
    "        self.layers = self.layers[:-1] + [layer] + [self.layers[-1]]\n",
    "        \n",
    "    def init_weights(self):\n",
    "        # each input goes straight to its dedicated neuron in the input layer\n",
    "        self.weights = [np.identity(self.input_size)]\n",
    "        \n",
    "        for i in range(len(self.layers) - 1):\n",
    "            this_layer = self.layers[i]\n",
    "            next_layer = self.layers[i+1]\n",
    "            self.weights.append(np.random.rand(next_layer.size, this_layer.size))\n",
    "            \n",
    "    def propagate(self, inputs: np.ndarray):\n",
    "        y = inputs\n",
    "        self.layers[0].y = inputs\n",
    "        i = 1\n",
    "        while i < len(self.layers):\n",
    "            current_layer = self.layers[i]\n",
    "            logits = np.dot(self.weights[i], y.reshape(len(y), 1))\n",
    "            y = np.array([sigmoid(logit) for logit in logits])\n",
    "            \n",
    "            current_layer.y = y\n",
    "            i += 1\n",
    "        return y\n",
    "    \n",
    "    def train(self, training_examples):\n",
    "        d_weights = self.backprop(training_examples)\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= d_weights[i]\n",
    "            \n",
    "    # calculate derivatives of the SME error function \n",
    "    #      E = 1/2 * sum(truth - prediction)^2\n",
    "    # in respect to the activation of each neuron\n",
    "    # returns list of vectors ([k][i] - derivative of activation of i-th neuron in k-th layer)\n",
    "    def activation_derivatives(self, t):\n",
    "        \n",
    "        # calculate dE/da for all neuron outputs\n",
    "        error_derivatives = []\n",
    "        \n",
    "        # pre-fill derivative table\n",
    "        for layer in self.layers:\n",
    "            error_derivatives.append(np.zeros(layer.size))\n",
    "        \n",
    "        # calculate output layer derivatives\n",
    "        output_error_derivatives = -(t - self.output_layer.y)\n",
    "        error_derivatives[-1] = output_error_derivatives\n",
    "        \n",
    "        # calculate hidden layer derivatives \n",
    "        for i, layer in reversed(list(enumerate(self.layers))[:-1]):\n",
    "            next_layer = self.layers[i+1]\n",
    "            next_layer_derivatives = error_derivatives[i+1]\n",
    "            \n",
    "            next_layer_term = next_layer.y * (1 - next_layer.y) * next_layer_derivatives # dE/dz \n",
    "            weight_matrix = self.weights[i+1] \n",
    "            \n",
    "            layer_derivatives = np.dot(next_layer_term, weight_matrix)\n",
    "            \n",
    "            error_derivatives[i] = layer_derivatives    \n",
    "        return error_derivatives\n",
    "    \n",
    "    def backprop(self, examples):\n",
    "        \n",
    "        # list of matrices to be added to each layer's weight matrix\n",
    "        d_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "            \n",
    "        # go over each example and add up the changes to be made to the weights between each layer\n",
    "        for example in examples:\n",
    "            x, t = example\n",
    "            y = self.propagate(x)\n",
    "            \n",
    "            derivs = self.activation_derivatives(t)\n",
    "            \n",
    "            for i, layer in enumerate(self.layers[:-1]):\n",
    "                next_layer = self.layers[i+1]\n",
    "                \n",
    "                next_layer_term = (next_layer.y * (1 - next_layer.y) * derivs[i + 1])\n",
    "                \n",
    "                layer_d_weights = learning_rate * transpose_mul_vectors(next_layer_term, layer.y)\n",
    "                \n",
    "                d_weights[i+1] -= layer_d_weights\n",
    "        return d_weights\n",
    "    \n",
    "    def example_error(self, example):\n",
    "        x, t = example\n",
    "        y = self.propagate(x)\n",
    "        return np.linalg.norm(t - y) ** 2\n",
    "    \n",
    "    def network_error(self, examples):\n",
    "        avg = 0\n",
    "        for example in examples:\n",
    "            avg += self.example_error(example)\n",
    "        return avg / (2 * len(examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "speaking-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "# couldn't figure out how to cleanly multiply vectors as matrices...\n",
    "def transpose_mul_vectors(v1, v2):\n",
    "    return np.dot(v1.reshape(len(v1), 1), v2.reshape(1, len(v2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "2d90ecd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1., 0.],\n",
       "        [0., 1.]]),\n",
       " array([[0.66615872, 0.62939505]])]"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mynet.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "federal-marriage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5046099])"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mynet = Network(2, 1)\n",
    "# mynet.add_layer(Layer(1))\n",
    "mynet.init_weights()\n",
    "mynet.propagate(training_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "6448899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mynet.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "structured-orlando",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([0.01795948, 0.01028967]), array([0.02824915])),\n",
       " (array([0.02421325, 0.06764977]), array([0.09186302])),\n",
       " (array([0.16177649, 0.25792974]), array([0.41970623])),\n",
       " (array([0.28496119, 0.21279294]), array([0.49775413])),\n",
       " (array([0.31648046, 0.22834824]), array([0.5448287]))]"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "training_data = []\n",
    "for i in range(10000):\n",
    "    x = random.uniform(0, .5)\n",
    "    y = random.uniform(0, .5)\n",
    "    training_data.append((np.array([x, y]), np.array([x+y])))\n",
    "training_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "laughing-violin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.07628795e-18])"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mynet.propagate(np.array([0.00001]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "amateur-bandwidth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023679231983302157"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mynet.network_error(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "female-lexington",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/mikhailandreev/code/learning/neuralnet', '/opt/homebrew/Cellar/python@3.9/3.9.7/Frameworks/Python.framework/Versions/3.9/lib/python39.zip', '/opt/homebrew/Cellar/python@3.9/3.9.7/Frameworks/Python.framework/Versions/3.9/lib/python3.9', '/opt/homebrew/Cellar/python@3.9/3.9.7/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload', '', '/opt/homebrew/lib/python3.9/site-packages', '/opt/homebrew/lib/python3.9/site-packages/IPython/extensions', '/Users/mikhailandreev/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "about-carpet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = np.array([1,2,3])\n",
    "v2 = np.array([1,0,1])\n",
    "myneuron = Neuron()\n",
    "myneuron.activate(v1, v2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}